{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6482b2bd-fef9-4565-8999-b41a32c2c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 15:02:59.296987: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-30 15:02:59.337157: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-30 15:02:59.338358: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-30 15:03:00.417945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearnex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearnex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patch_sklearn\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     18\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearnex'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486f605-19fd-461d-9ff9-45295fda19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(\"standard-ocr-dataset/data\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff9a53-fce6-40fe-8fbb-ef68b4e38375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images_arr, imageWidth, imageHeight):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img.reshape(imageWidth, imageHeight), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591f62b-41b1-46a0-8a5d-c4cc0c0a9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fun(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY, cv2.CV_8UC1)\n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    img = img.reshape((28, 28, 1))\n",
    "    thresh = cv2.adaptiveThreshold(img, 255, 1, 1, 11, 2)\n",
    "    img = np.where(img > 140, 1, 0)\n",
    "    img = img / 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086008f1-7618-4979-8182-a525ac48bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Intel threading layer for better performance\n",
    "patch_sklearn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5854b02-55e2-41cb-a083-b81591fb2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image_gen = ImageDataGenerator(\n",
    "    rescale=1/255.0,\n",
    "    rotation_range=2,\n",
    "    width_shift_range=.1,\n",
    "    height_shift_range=.1,\n",
    "    zoom_range=0.1,\n",
    "    shear_range=2,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "normal_image_gen = ImageDataGenerator(\n",
    "    rescale=1/255.0,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9555ee7-cb80-4b67-a739-6b39d9bc94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = augmented_image_gen.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=\"./training_data\",\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    class_mode=\"categorical\",\n",
    "    seed=65657867,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data_gen = normal_image_gen.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=\"./testing_data\",\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    class_mode=\"categorical\",\n",
    "    seed=65657867,\n",
    "    subset='validation'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834480b-0059-4e17-8a15-5b03a0abaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_images, _ = next(train_data_gen)\n",
    "plot_images(sample_training_images[:7], IMG_WIDTH, IMG_HEIGHT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483a67c-4fbf-45dc-9545-8336e7fdd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(36, activation='softmax'))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f7391-1f14-417b-9bbf-4324c167648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ca2fd-c9ed-485b-bcb0-3d57104e19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intel-specific configuration options for TensorFlow\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "os.environ['TF_ENABLE_INTEL_OPS'] = '1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936c435-a08d-4534-a680-c47ba04c571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStop_callback = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "checkpoint_dir = 'path/to/checkpoints'  # Specify the checkpoint directory\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model_checkpoint.h5')\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Define the callback for model checkpointing\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss', mode='min', save_best_only=True\n",
    ")\n",
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "my_callbacks = [EarlyStop_callback, checkpoint, lr]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4c467-a9a0-4f3b-8645-3b057cadb86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=train_data_gen.samples // batch_size,\n",
    "    epochs=32,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=val_data_gen.samples // batch_size,\n",
    "    callbacks=my_callbacks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9ac35-596c-48e7-b22c-547cc0e6afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(32)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1cb70-3e8f-444d-9045-f9b66dd019c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_image_gen = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True\n",
    ")\n",
    "\n",
    "test_data_gen = normal_image_gen.flow_from_directory(\n",
    "    batch_size=5193,\n",
    "    directory=\"./testing_data\",\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7ec0b-a819-4e45-aa81-7b8520eeac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "test_images, test_labels = next(test_data_gen)\n",
    "filenames = test_data_gen.filenames\n",
    "test_pred = model.predict(test_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452db768-2aee-4230-836d-69f075ad51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = tf.argmax(test_labels, 1)\n",
    "predicted_labels = tf.argmax(test_pred, 1)\n",
    "confusion_matrix(true_labels, predicted_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e23e8-4fe4-4db4-ad5f-b9fbb67177a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('testing_data/N/28333.png', 0)\n",
    "img = img / 255\n",
    "img = cv2.resize(img, (28, 28))\n",
    "img = img.reshape((1, 28, 28, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ff4ec-f440-43a2-bdcb-09a92b7549a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421e4ba-8c8f-42fe-9813-1fd9907df891",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argmax(model.predict(img), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543f084-82a6-4920-bb88-330ee6a04d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow (AI kit)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
